{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMKkIJgppQuQQMZvWZLOB8n"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Early Neural Networks & Machine Learning (1960s - 1980s)**\n"],"metadata":{"id":"xxSePycZOkcH"}},{"cell_type":"markdown","source":["\n","## **Introduction**\n","During the period from the **1960s to the 1980s**, artificial intelligence research saw significant developments in **machine learning and neural networks**. After the limitations of symbolic AI became apparent, researchers shifted towards **connectionist approaches**, probabilistic reasoning, and optimization techniques.\n","\n","The following key advancements shaped modern machine learning and deep learning.\n","\n","---\n"],"metadata":{"id":"OGps6aA1Oke5"}},{"cell_type":"markdown","source":["\n","## **1. Production Rule Systems (OPS5)**\n","### **Overview**\n","- **Production rule systems** are a form of **symbolic AI** that rely on **if-then rules** to guide decision-making.\n","- **OPS5**, developed at **Carnegie Mellon University** in the late 1970s, was the first widely used **production system language**.\n","- It used **forward chaining inference** to match rules to facts and make logical deductions.\n","\n","### **How It Differs from Other Approaches**\n","- Unlike **neural networks**, which learn patterns from data, **OPS5 used explicit rules**.\n","- It was a **deterministic system**, whereas probabilistic models like **Bayesian networks** (discussed later) handled uncertainty.\n","\n","### **Challenges & Limitations**\n","- **Scalability Issues**: Required **manually encoding** knowledge, leading to **knowledge bottleneck**.\n","- **Brittleness**: Could not **generalize** beyond predefined rules.\n","- **Lack of Adaptability**: Unlike **neural networks** or **genetic algorithms**, it could not **learn** from data.\n","\n","---\n"],"metadata":{"id":"sJ735UK1Okh8"}},{"cell_type":"markdown","source":["\n","## **2. Backpropagation Neural Networks (1980s)**\n","### **Overview**\n","- Backpropagation (BP) is a **supervised learning** algorithm that enabled **multi-layer perceptrons (MLPs)** to train efficiently.\n","- It was formally introduced by **Rumelhart, Hinton, and Williams (1986)**.\n","- Uses **gradient descent** to minimize errors by **adjusting weights** based on error signals.\n","\n","### **How It Differs from Other Neural Networks**\n","- **Compared to Perceptrons (1957)**: Can model **non-linear functions**, unlike single-layer perceptrons.\n","- **Compared to Hopfield Networks (1982)**: Backpropagation networks are used for **classification**, while Hopfield networks are designed for **associative memory**.\n","- **Compared to Boltzmann Machines (1985)**: BP networks use **deterministic training**, while Boltzmann Machines use **stochastic (probabilistic) learning**.\n","\n","### **Challenges & Limitations**\n","- **Vanishing Gradient Problem**: Deep networks suffered from **weak gradient signals** for deeper layers.\n","- **Slow Convergence**: Training took **a long time** in early hardware.\n","- **Need for Large Data**: Required **significant amounts of labeled data**.\n","\n","---\n"],"metadata":{"id":"GraNSFiWOkkv"}},{"cell_type":"markdown","source":["\n","## **3. Hopfield Networks (1982)**\n","### **Overview**\n","- Developed by **John Hopfield**, Hopfield Networks are **recurrent neural networks (RNNs)** used for **associative memory**.\n","- Stores **patterns** as stable states in a **dynamic system**.\n","\n","### **How It Differs from Other Neural Networks**\n","- **Compared to Backpropagation Networks**: Hopfield Networks are **not used for classification** but for **content-addressable memory**.\n","- **Compared to Boltzmann Machines**: Hopfield Networks are **deterministic**, whereas Boltzmann Machines introduce **stochasticity**.\n","\n","### **Challenges & Limitations**\n","- **Capacity Limitations**: Could only store a **limited number of patterns**.\n","- **Spurious States**: Network sometimes settled into **incorrect local minima**.\n","- **Not Scalable**: As the number of neurons increased, the network **became unstable**.\n","\n","---\n"],"metadata":{"id":"f9IDeviVOknf"}},{"cell_type":"markdown","source":["\n","## **4. Boltzmann Machines (1985)**\n","### **Overview**\n","- Introduced by **Geoffrey Hinton and Terry Sejnowski**.\n","- A **stochastic neural network** that uses **energy-based models** to find patterns.\n","- Uses **probability distributions** and **simulated annealing** to optimize solutions.\n","\n","### **How It Differs from Other Neural Networks**\n","- **Compared to Hopfield Networks**: Uses **stochastic (random) updates**, making it better at **avoiding local minima**.\n","- **Compared to Backpropagation**: Does not require labeled data; can perform **unsupervised learning**.\n","\n","### **Challenges & Limitations**\n","- **Slow Training**: Requires a **large number of iterations** to converge.\n","- **Computationally Expensive**: Scaling to **large datasets** was impractical in the 1980s.\n","- **Difficult to Interpret**: Unlike symbolic AI, neural networks are **black boxes**.\n","\n","---\n"],"metadata":{"id":"cHrFXqu-Okq1"}},{"cell_type":"markdown","source":["\n","## **5. Self-Organizing Maps (Kohonen, 1982)**\n","### **Overview**\n","- Developed by **Teuvo Kohonen**.\n","- A type of **unsupervised learning algorithm** that organizes data into a **low-dimensional space**.\n","- Often used for **clustering and visualization**.\n","\n","### **How It Differs from Other Neural Networks**\n","- **Compared to Backpropagation**: Does not require labeled data, making it more like a clustering algorithm.\n","- **Compared to Boltzmann Machines**: Uses **deterministic neighborhood updates** rather than **stochastic probability distributions**.\n","\n","### **Challenges & Limitations**\n","- **Curse of Dimensionality**: Struggles with **high-dimensional data**.\n","- **Fixed Topology**: The structure is **predefined**, limiting adaptability.\n","- **Sensitive to Initial Parameters**: Requires careful **parameter tuning**.\n","\n","---\n"],"metadata":{"id":"8Z7XDqjWOktS"}},{"cell_type":"markdown","source":["\n","## **6. Genetic Algorithms (Popular in 1980s)**\n","### **Overview**\n","- Inspired by **biological evolution** (Holland, 1975; Goldberg, 1989).\n","- Uses **mutation, crossover, and selection** to evolve optimal solutions.\n","\n","### **How It Differs from Other Machine Learning Approaches**\n","- **Compared to Neural Networks**: Does not rely on gradient descent; instead, it explores the **entire solution space**.\n","- **Compared to Bayesian Networks**: Evolutionary-based, rather than probabilistic.\n","\n","### **Challenges & Limitations**\n","- **Computational Cost**: Needs many **generations** to find good solutions.\n","- **No Theoretical Guarantees**: May not converge to the **global optimum**.\n","- **Hard to Tune Parameters**: Evolution strategies are **problem-specific**.\n","\n","---\n"],"metadata":{"id":"uhpulHN_Okv2"}},{"cell_type":"markdown","source":["\n","## **7. Bayesian Networks (1985)**\n","### **Overview**\n","- Introduced by **Judea Pearl**.\n","- A **probabilistic graphical model** that encodes **dependencies between variables**.\n","\n","### **How It Differs from Other Machine Learning Methods**\n","- **Compared to Neural Networks**: Explicitly models **uncertainty**, whereas neural networks **learn representations**.\n","- **Compared to Markov Decision Processes**: Uses **directed acyclic graphs (DAGs)** instead of **sequential decision-making**.\n","\n","### **Challenges & Limitations**\n","- **Complexity**: Computationally expensive for **large networks**.\n","- **Difficult Parameter Estimation**: Requires **expert knowledge**.\n","\n","---\n"],"metadata":{"id":"5eYM6ovGOkyy"}},{"cell_type":"markdown","source":["\n","## **8. Markov Decision Processes (MDPs)**\n","### **Overview**\n","- A framework for **sequential decision-making under uncertainty**.\n","- Uses **states, actions, rewards, and transition probabilities**.\n","\n","### **How It Differs from Other Approaches**\n","- **Compared to Bayesian Networks**: MDPs model **dynamic decisions** rather than static relationships.\n","- **Compared to Genetic Algorithms**: MDPs rely on **policy optimization**, whereas GAs use **evolution**.\n","\n","### **Challenges & Limitations**\n","- **Computationally Expensive**: Solving large MDPs requires **reinforcement learning**.\n","- **Difficulty in Real-World Application**: Often **simplifies** real-world **decision-making**.\n","\n","---\n"],"metadata":{"id":"DQH68dJeOk1n"}},{"cell_type":"markdown","source":["\n","## **9. Hidden Markov Models (HMMs)**\n","### **Overview**\n","- A **probabilistic model** for **time-series and sequential data**.\n","- Used in **speech recognition, financial modeling, and bioinformatics**.\n","\n","### **How It Differs from Other Approaches**\n","- **Compared to Bayesian Networks**: Focuses on **sequences** rather than **static probability distributions**.\n","- **Compared to Neural Networks**: Uses **probabilistic reasoning** rather than feature learning.\n","\n","### **Challenges & Limitations**\n","- **Scaling to Large Data**: Training requires **complex inference techniques**.\n","- **Limited Expressivity**: Struggles with **long-term dependencies**.\n"],"metadata":{"id":"MPPQSPaIOk4Z"}},{"cell_type":"markdown","source":[],"metadata":{"id":"MWsjY1_aOk7Y"}},{"cell_type":"markdown","source":[],"metadata":{"id":"1R-tvfhzOk-G"}},{"cell_type":"markdown","source":[],"metadata":{"id":"dmGub-PQOlA-"}},{"cell_type":"markdown","source":[],"metadata":{"id":"znve-te5OlEe"}}]}