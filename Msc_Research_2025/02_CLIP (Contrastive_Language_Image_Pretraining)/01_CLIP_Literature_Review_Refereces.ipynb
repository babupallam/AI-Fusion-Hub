{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# References:\n",
        "  - CLIP Code: https://github.com/openai/CLIP\n",
        "  - CLIP Paper: https://arxiv.org/abs/2103.00020\n",
        "  - CLIP Blog: https://openai.com/index/clip/\n",
        "  - CLIP Model Card: https://github.com/openai/CLIP/blob/main/model-card.md\n",
        "  - CLIP Colab: https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb\n",
        "  "
      ],
      "metadata": {
        "id": "ud_PN3affjhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "qqeRlFLIi5qL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLIP Code: https://github.com/openai/CLIP\n",
        "  "
      ],
      "metadata": {
        "id": "kq0U2lfggzSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ei5QAzZRgzN4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLIP Paper: https://arxiv.org/abs/2103.00020\n",
        "  "
      ],
      "metadata": {
        "id": "SlAph0TsfmnK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Bdxw2SAvfmnL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BnY6xYIDfmnM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6fvbMsaCfmnM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FmjuaVk5fmnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iYrl1-GsfmnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "1ZK6zRQmfmnO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLIP Blog: https://openai.com/index/clip/\n",
        "  "
      ],
      "metadata": {
        "id": "h2Z57ditfmtx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Summary of CLIP: Connecting Text and Images**  \n",
        "\n",
        "#### **üîπ Introduction**\n",
        "- CLIP (Contrastive Language‚ÄìImage Pretraining) is a **neural network** that learns **visual concepts** from **natural language supervision**.\n",
        "- It enables **zero-shot** image classification, similar to how **GPT-2 & GPT-3** perform zero-shot text generation.\n",
        "- Unlike traditional vision models, CLIP can classify images **without task-specific training**.\n",
        "\n",
        "---\n",
        "\n",
        "#### **üîπ Problems CLIP Solves**\n",
        "1. **Expensive & Narrow Datasets:**  \n",
        "   - Traditional vision datasets require **manual labeling** (e.g., ImageNet took **25,000 workers**).  \n",
        "   - CLIP learns from **publicly available image-text pairs** from the internet.\n",
        "  \n",
        "2. **Limited Generalization:**  \n",
        "   - Standard vision models only work for one task and require **fine-tuning** for new ones.  \n",
        "   - CLIP can **generalize** across **multiple tasks** using only natural language labels.\n",
        "\n",
        "3. **Poor Real-World Performance:**  \n",
        "   - Models optimized for benchmarks often **fail in real-world scenarios**.  \n",
        "   - CLIP‚Äôs zero-shot evaluation is **more representative of real-world robustness**.\n",
        "\n",
        "---\n",
        "\n",
        "#### **üîπ CLIP's Approach**\n",
        "- Uses **internet-scale** text-image pairs to train a **contrastive learning** model.\n",
        "- Learns to **match images with the most relevant text description** from a pool of 32,768 options.\n",
        "- Can be adapted to **any classification task** by providing text descriptions (e.g., *‚ÄúA photo of a cat‚Äù* vs *‚ÄúA photo of a dog‚Äù*).\n",
        "\n",
        "---\n",
        "\n",
        "#### **üîπ Performance Comparison**  \n",
        "CLIP **outperforms traditional models** in generalization, even with the same ImageNet accuracy:\n",
        "\n",
        "| Dataset | ResNet-101 Accuracy | CLIP Accuracy |\n",
        "|---------|---------------------|--------------|\n",
        "| **ImageNet** | 76.2% | 76.2% |\n",
        "| **ImageNet-V2** | 64.3% | 70.1% |\n",
        "| **ObjectNet** | 32.6% | 72.3% |\n",
        "| **ImageNet-Sketch** | 25.2% | 60.2% |\n",
        "| **ImageNet-Adversarial** | 2.7% | 77.1% |\n",
        "\n",
        "‚û° **CLIP is more robust** to different environments & variations.\n",
        "\n",
        "---\n",
        "\n",
        "#### **üîπ CLIP vs Traditional Vision Models**\n",
        "| Feature | Traditional Vision Models | CLIP |\n",
        "|---------|-------------------------|------|\n",
        "| **Training Data** | Manually labeled datasets (ImageNet) | Publicly available image-text pairs |\n",
        "| **Flexibility** | Requires fine-tuning for new tasks | Works on **any classification task** with text prompts |\n",
        "| **Generalization** | Performs well on specific benchmarks | Performs well across **many real-world datasets** |\n",
        "| **Training Efficiency** | Requires extensive labeled data | Uses contrastive learning for faster training |\n",
        "\n",
        "---\n",
        "\n",
        "#### **üîπ Key Innovations**\n",
        "1. **Contrastive Learning** ‚Äì Learns representations by distinguishing matching vs. non-matching text-image pairs.\n",
        "2. **Vision Transformer (ViT)** ‚Äì Uses a transformer model for image processing instead of CNNs.\n",
        "3. **Zero-Shot Transfer** ‚Äì No need to retrain; just provide text labels for classification.\n",
        "\n",
        "---\n",
        "\n",
        "#### **üîπ CLIP in Action (Zero-Shot Predictions)**\n",
        "Examples of how CLIP performs **zero-shot classification**:\n",
        "\n",
        "| Dataset | CLIP's Prediction |\n",
        "|---------|------------------|\n",
        "| **Food-101** | *\"A photo of guacamole\"* (90.1% confidence) |\n",
        "| **ObjectNet** | *\"A television studio\"* (90.2% confidence) |\n",
        "| **YouTube-BB** | *\"An airplane, a person\"* (89.0% confidence) |\n",
        "| **EuroSAT** | *\"A centered satellite photo of annual crop land\"* (46.5% confidence) |\n",
        "\n",
        "---\n",
        "\n",
        "#### **üîπ Limitations**\n",
        "1. **Struggles with Abstract Tasks:**  \n",
        "   - CLIP is good at recognizing common objects but **fails at counting objects** or understanding **relative spatial positions**.\n",
        "  \n",
        "2. **Prompt Sensitivity:**  \n",
        "   - Performance depends on **exact wording of text prompts** (e.g., \"a cat\" vs. \"a cute kitty\" might yield different results).\n",
        "  \n",
        "3. **Fine-Grained Classification Issues:**  \n",
        "   - Struggles with differentiating **car models, aircraft types, and flower species**.\n",
        "\n",
        "---\n",
        "\n",
        "#### **üîπ Key Takeaways**\n",
        "1. **Highly Efficient:** Uses **contrastive learning + transformers** for faster training.\n",
        "2. **Flexible & General:** Can perform **many vision tasks** without fine-tuning.\n",
        "3. **More Robust:** Performs better on **real-world datasets** than traditional models.\n",
        "4. **Zero-Shot Capabilities:** No need for labeled data; simply provide text prompts.\n",
        "5. **Future of AI Vision:** Combines **language & vision** for a more human-like understanding of images.\n"
      ],
      "metadata": {
        "id": "ABIcVgSOfmty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nicUAcK-fmty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CLIP Model: Detailed Specification & Performance Overview**  \n",
        "\n",
        "### **1Ô∏è‚É£ Model Overview**\n",
        "**CLIP (Contrastive Language-Image Pretraining)** is an AI model developed by OpenAI that learns **visual concepts** from **natural language supervision**. It is trained to associate images with text descriptions and can perform zero-shot classification tasks.\n",
        "\n",
        "#### **üîπ Model Architectures Used in CLIP**\n",
        "- **Vision Backbone:** ResNet-50, ResNet-101, or Vision Transformer (ViT)\n",
        "- **Text Encoder:** Transformer similar to GPT\n",
        "\n",
        "---\n",
        "\n",
        "### **2Ô∏è‚É£ CLIP's Performance Compared to ResNet-101**\n",
        "| **Dataset**           | **ResNet-101 Accuracy** | **CLIP Accuracy (ViT-L/14)** |\n",
        "|-----------------------|------------------------|------------------------------|\n",
        "| **ImageNet**          | 76.2%                   | 76.2%                        |\n",
        "| **ImageNet-V2**       | 64.3%                   | 70.1%                        |\n",
        "| **ImageNet Rendition**| 37.7%                   | 88.9%                        |\n",
        "| **ObjectNet**         | 32.6%                   | 72.3%                        |\n",
        "| **ImageNet-Sketch**   | 25.2%                   | 60.2%                        |\n",
        "| **ImageNet-Adversarial** | 2.7%                | 77.1%                        |\n",
        "\n",
        "#### **üîπ Key Findings**\n",
        "1. **CLIP matches ResNet-101 on ImageNet (76.2%)** but **generalizes better** across datasets.\n",
        "2. **CLIP outperforms ResNet-101** in real-world datasets where images have different styles, adversarial modifications, or sketches.\n",
        "3. **Robustness Improvement:** CLIP closes the **\"robustness gap\"** by up to **75%** on datasets like ObjectNet.\n",
        "\n",
        "---\n",
        "\n",
        "### **3Ô∏è‚É£ Model Specifications**\n",
        "#### **üîπ Vision Encoder**\n",
        "CLIP's vision encoder can be either:\n",
        "- **ResNet-50/101:** Traditional convolutional neural network (CNN) for feature extraction.\n",
        "- **Vision Transformer (ViT-B/32, ViT-L/14):** A transformer-based model that processes images as a sequence of patches.\n",
        "\n",
        "#### **üîπ Text Encoder**\n",
        "- **Transformer-based text model**, similar to GPT.\n",
        "- Converts text into **512-dimensional feature vectors**.\n",
        "\n",
        "#### **üîπ Training Data**\n",
        "- **400 million image-text pairs** collected from the **internet**.\n",
        "- Uses **contrastive learning**: It learns to associate images with relevant text while distinguishing incorrect pairs.\n",
        "\n",
        "#### **üîπ Zero-Shot Learning**\n",
        "- Can classify **new images** without explicit training by using **text prompts**.\n",
        "- Works similarly to GPT-3‚Äôs ability to perform NLP tasks without fine-tuning.\n",
        "\n",
        "---\n",
        "\n",
        "### **4Ô∏è‚É£ How CLIP Works?**\n",
        "1. **Image Processing:**\n",
        "   - Input images are processed using either **ResNet-101** or **Vision Transformer**.\n",
        "   - The images are mapped into a **feature space**.\n",
        "\n",
        "2. **Text Processing:**\n",
        "   - Input text descriptions are tokenized and processed by a **transformer-based text model**.\n",
        "   - Text features are mapped to the **same feature space** as images.\n",
        "\n",
        "3. **Similarity Matching:**\n",
        "   - CLIP measures how well an image matches a text prompt.\n",
        "   - Uses **cosine similarity** to compute the best match.\n",
        "\n",
        "4. **Zero-Shot Classification:**\n",
        "   - Instead of training for each classification task separately, CLIP assigns labels by comparing images to **text prompts**.\n",
        "\n",
        "---\n",
        "\n",
        "### **5Ô∏è‚É£ Comparison Between CLIP and ResNet-101**\n",
        "| Feature                   | ResNet-101                | CLIP (ViT-L/14)             |\n",
        "|---------------------------|--------------------------|-----------------------------|\n",
        "| **Training Data**         | 1.2M labeled images (ImageNet) | 400M image-text pairs (Internet) |\n",
        "| **Task-Specific Training**| Required for each task   | Not required (Zero-Shot)    |\n",
        "| **Generalization**        | Poor outside ImageNet    | Excellent generalization    |\n",
        "| **Training Method**       | Supervised learning      | Contrastive learning        |\n",
        "| **Backbone**              | CNN (ResNet)             | Vision Transformer (ViT)    |\n",
        "| **Text Understanding**    | No text processing       | Learns from text captions   |\n",
        "| **Robustness**            | Struggles with variations | Strong against adversarial images, sketches, and different backgrounds |\n",
        "| **Compute Efficiency**    | Lower                    | More efficient (uses transformers) |\n",
        "\n",
        "---\n",
        "\n",
        "### **6Ô∏è‚É£ Model Variants**\n",
        "| **Model**  | **Parameters** | **Image Encoder** | **Text Encoder** |\n",
        "|-----------|--------------|-----------------|----------------|\n",
        "| RN50      | 102M         | ResNet-50       | Transformer   |\n",
        "| RN101     | 163M         | ResNet-101      | Transformer   |\n",
        "| ViT-B/32  | 149M         | Vision Transformer (Base) | Transformer |\n",
        "| ViT-B/16  | 151M         | Vision Transformer (Base) | Transformer |\n",
        "| ViT-L/14  | 428M         | Vision Transformer (Large) | Transformer |\n",
        "\n",
        "üîπ **Best model:** **ViT-L/14** ‚Üí **Most accurate & generalizable**.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "CYEGt_Jmfmtz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hcEpgXW6fmtz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GvCa6Zq4fmt0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "YD_KIzInfmt0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLIP Model Card: https://github.com/openai/CLIP/blob/main/model-card.md\n",
        "  "
      ],
      "metadata": {
        "id": "3WMbNlfVfmxz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Summary of CLIP Model Card**  \n",
        "\n",
        "### **1Ô∏è‚É£ Model Overview**\n",
        "- **CLIP (Contrastive Language-Image Pretraining)** is a multimodal model developed by **OpenAI**.\n",
        "- Designed to **understand robustness in vision tasks** and **perform zero-shot image classification**.\n",
        "- Not intended for **direct deployment**; requires further study before use in real-world applications.\n",
        "\n",
        "---\n",
        "\n",
        "### **2Ô∏è‚É£ Model Specifications**\n",
        "- **Release Date:** January 2021\n",
        "- **Architecture:**\n",
        "  - Image Encoder: **ResNet-50 / Vision Transformer (ViT)**\n",
        "  - Text Encoder: **Masked Self-Attention Transformer**\n",
        "- **Training Method:** Contrastive loss to match image-text pairs.\n",
        "- **Model Variants:** ViT-B/32, RN50, RN101, RN50x4, RN50x16, RN50x64, ViT-B/16, ViT-L/14, ViT-L/14@336px.\n",
        "\n",
        "---\n",
        "\n",
        "### **3Ô∏è‚É£ Model Usage**\n",
        "#### **üîπ Intended Uses**\n",
        "- **AI Research**: To study robustness, generalization, and model biases.\n",
        "- **Zero-shot Image Classification**: Matching images to text descriptions without direct training.\n",
        "- **Interdisciplinary Studies**: Evaluating societal impacts of multimodal models.\n",
        "\n",
        "#### **üîπ Out-of-Scope Uses**\n",
        "- **Commercial or Deployed Use**: The model has not been tested for direct application.\n",
        "- **Surveillance & Facial Recognition**: Prohibited due to ethical concerns.\n",
        "- **Non-English Tasks**: CLIP is only trained in English and may not generalize to other languages.\n",
        "\n",
        "---\n",
        "\n",
        "### **4Ô∏è‚É£ Training Data**\n",
        "- **Source:** Publicly available **image-caption datasets**.\n",
        "- **Data Collection:** Crawled from internet sources like YFCC100M.\n",
        "- **Bias Risk:** Skews towards **developed nations, younger, male users** due to internet data sources.\n",
        "- **Ethical Filtering:** Sites with **violent or explicit content** were excluded.\n",
        "\n",
        "---\n",
        "\n",
        "### **5Ô∏è‚É£ Performance & Limitations**\n",
        "#### **üîπ Benchmarks**\n",
        "Evaluated on **30+ datasets** including:\n",
        "- **Object Classification:** ImageNet, CIFAR10, CIFAR100, Food101\n",
        "- **Scene Recognition:** SUN397, Stanford Cars, FGVC Aircraft\n",
        "- **Text & OCR:** MNIST, SVHN, IIIT5K, Hateful Memes\n",
        "- **Complex Reasoning:** CLEVR Counting, KITTI Distance\n",
        "- **Robustness Testing:** ImageNet-A, ImageNet-R, ImageNet Sketch, ObjectNet\n",
        "\n",
        "#### **üîπ Limitations**\n",
        "- **Struggles with fine-grained classification** (e.g., distinguishing car models, aircraft types).\n",
        "- **Fails at counting objects** and complex visual reasoning.\n",
        "- **Bias Concerns:** Performance varies based on dataset labels, class design, and race/gender accuracy.\n",
        "- **Fairness Issues:** Differences in accuracy across racial and gender groups.\n",
        "\n",
        "---\n",
        "\n",
        "### **6Ô∏è‚É£ Bias & Fairness**\n",
        "- **Bias Exists in Model Predictions**: Results vary depending on **race, gender, and dataset categories**.\n",
        "- **Fairface Dataset Evaluation:**\n",
        "  - **>96% accuracy in gender classification**\n",
        "  - **~93% accuracy in race classification**\n",
        "  - **~63% accuracy in age classification**\n",
        "- **Ethical Considerations:** Model may **exhibit denigration risks** based on class choices.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "IOmfVhLnfmx0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "V2ShL6tgfmx0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "btOD9TWyfmx0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "T3XHJqA3fmx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mrUtf0cEfmx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "uQYMUF8efmx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLIP Colab: https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb\n",
        "  "
      ],
      "metadata": {
        "id": "LX8RE04gfm09"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7dzT3-Dnfm0-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4q0gDlwffm0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1YHav351fm0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1zcGjcuqfm1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "k1ZLlg_Dfm1B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "DopUtsi1fm1B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Open CLIP: https://github.com/mlfoundations/open_clip"
      ],
      "metadata": {
        "id": "-XhBz8wNfm5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KEo3ilEKfm5e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "K3Exd1PRfm5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "58smobgkfm5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4eBCeVugfm5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BI0RHXUNfm5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "fwXd-ZT1fm5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face implementation of CLIP: https://huggingface.co/docs/transformers/model_doc/clip"
      ],
      "metadata": {
        "id": "EprWEYqUjL1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UiFnRKuIjLwn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4KGum6ryjLtz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "p7Ce6XgpjLqt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XAqQRFpsjLom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FiP1VEqujLkH"
      }
    }
  ]
}